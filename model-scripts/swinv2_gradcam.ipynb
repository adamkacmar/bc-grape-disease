{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff2be68",
   "metadata": {},
   "source": [
    "# Grad-CAM for SwinV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6941b80",
   "metadata": {},
   "source": [
    "This notebook uses adjusted code from: https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41fa3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "from transformers import Swinv2ForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "434045b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = \"../dataset/test/Healthy/0c1667a2-61d7-4dee-b4d9-0d141a1ceb20___Mt.N.V_HL 9127_new30degFlipLR.JPG\"\n",
    "augmented_path = \"../augmented-dataset/test/Black Rot/0aff8add-93ad-4099-97ae-23515744e620___FAM_B.Rot 0748_flipLR_aug2.png\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dirs = [\"../final-models/hp-vit-base\", \"../final-models/aug-hp-vit-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(x):\n",
    "    B, N, C = x.size()\n",
    "    side = int(N ** 0.5)\n",
    "    return x.view(B, side, side, C).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = Swinv2ForImageClassification.from_pretrained(\n",
    "        \"microsoft/swinv2-base-patch4-window8-256\",\n",
    "        num_labels=4,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load('../final-models/aug-hp-swinv2-base/model_lr1e-05_bs32.pth', map_location=device))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "688f1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_img_display shape: (256, 256, 3) range: 0.0 1.0\n",
      "grayscale_cam shape: (256, 256) range: 0.0 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# target_layer = model.swinv2.encoder.layers[-1].blocks[-1].layernorm_after\n",
    "target_layer = model.swinv2.layernorm\n",
    "\n",
    "class SwinV2Wrapper(torch.nn.Module):\n",
    "    def __init__(self, swin_model):\n",
    "        super().__init__()\n",
    "        self.model = swin_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "    \n",
    "wrapped_model = SwinV2Wrapper(model)\n",
    "\n",
    "cam = GradCAM(model=wrapped_model, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "\n",
    "rgb_img = cv2.imread(augmented_path)[:, :, ::-1]\n",
    "rgb_img = cv2.resize(rgb_img, (256, 256))\n",
    "rgb_img_display = np.float32(rgb_img) / 255.0\n",
    "input_tensor = preprocess_image(rgb_img_display.copy() , mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5]).to(device)\n",
    "\n",
    "targets = None\n",
    "\n",
    "cam.batch_size = 16\n",
    "\n",
    "# grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth=True)[0]\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "\n",
    "\n",
    "cam_image = show_cam_on_image(rgb_img_display, grayscale_cam, use_rgb=False)\n",
    "cv2.imwrite(f'grad_cam.jpg', cam_image)\n",
    "\n",
    "print(\"rgb_img_display shape:\", rgb_img_display.shape, \"range:\", rgb_img_display.min(), rgb_img_display.max())\n",
    "print(\"grayscale_cam shape:\", grayscale_cam.shape, \"range:\", grayscale_cam.min(), grayscale_cam.max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

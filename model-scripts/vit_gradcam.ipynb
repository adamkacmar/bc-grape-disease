{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0844da53",
   "metadata": {},
   "source": [
    "# Grad-CAM for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef13fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568a3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = \"../dataset/test/Healthy/0c1667a2-61d7-4dee-b4d9-0d141a1ceb20___Mt.N.V_HL 9127_new30degFlipLR.JPG\"\n",
    "augmented_path = \"../augmented-dataset/test/Black Rot/0aff8add-93ad-4099-97ae-23515744e620___FAM_B.Rot 0748_flipLR_aug2.png\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dirs = [\"../final-models/hp-vit-base\", \"../final-models/aug-hp-vit-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe78fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5336\\2319254178.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../final-models/hp-vit-base/model_lr0.0001_bs16.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x ViTLayer(\n",
      "      (attention): ViTSdpaAttention(\n",
      "        (attention): ViTSdpaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): ViTSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): ViTIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): ViTOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python vit_gradcam.py --image-path <path_to_image>\n",
    "    Example usage of using cam-methods on a VIT network.\n",
    "\n",
    "    \"\"\"\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        num_labels=4, \n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the fine-tuned weights from your checkpoint\n",
    "    model.load_state_dict(torch.load('../final-models/hp-vit-base/model_lr0.0001_bs16.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    print(model.vit.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a7245",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     class_idx \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# AblationCAM and ScoreCAM have batched implementations.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# You can override the internal batch size for faster computation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:856\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    854\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    867\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:639\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    633\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m    635\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    636\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    637\u001b[0m )\n\u001b[1;32m--> 639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    647\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:468\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    461\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    462\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    463\u001b[0m         hidden_states,\n\u001b[0;32m    464\u001b[0m         layer_head_mask,\n\u001b[0;32m    465\u001b[0m         output_attentions,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 468\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32mc:\\adam_kacmar\\env\\Lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:23\u001b[0m, in \u001b[0;36mActivationsAndGradients.save_activation\u001b[1;34m(self, module, input, output)\u001b[0m\n\u001b[0;32m     20\u001b[0m activation \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mappend(activation\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach())\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36mreshape_transform\u001b[1;34m(tensor, height, width)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape_transform\u001b[39m(tensor, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m      3\u001b[0m                                       height, width, tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Bring the channels to the first dimension,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# like in CNNs.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "target_layer = model.vit.encoder.layer[11].output\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "# cam = GradCAM(model=model, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "\n",
    "rgb_img = cv2.imread(original_path, 1)[:, :, ::-1]\n",
    "rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5]).to(device)\n",
    "\n",
    "# If None, returns the map for the highest scoring category.\n",
    "# Otherwise, targets the requested category.\n",
    "targets = None\n",
    "\n",
    "\n",
    "# AblationCAM and ScoreCAM have batched implementations.\n",
    "# You can override the internal batch size for faster computation.\n",
    "cam.batch_size = 32\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                    targets=targets,\n",
    "                    eigen_smooth=True,\n",
    "                    aug_smooth=True)\n",
    "\n",
    "# Here grayscale_cam has only one image in the batch\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "cam_image = show_cam_on_image(rgb_img, grayscale_cam[0], use_rgb=True)\n",
    "cv2.imwrite(f'grad_cam.jpg', cam_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b90755da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vit\n",
      "vit.embeddings\n",
      "vit.embeddings.patch_embeddings\n",
      "vit.embeddings.patch_embeddings.projection\n",
      "vit.embeddings.dropout\n",
      "vit.encoder\n",
      "vit.encoder.layer\n",
      "vit.encoder.layer.0\n",
      "vit.encoder.layer.0.attention\n",
      "vit.encoder.layer.0.attention.attention\n",
      "vit.encoder.layer.0.attention.attention.query\n",
      "vit.encoder.layer.0.attention.attention.key\n",
      "vit.encoder.layer.0.attention.attention.value\n",
      "vit.encoder.layer.0.attention.attention.dropout\n",
      "vit.encoder.layer.0.attention.output\n",
      "vit.encoder.layer.0.attention.output.dense\n",
      "vit.encoder.layer.0.attention.output.dropout\n",
      "vit.encoder.layer.0.intermediate\n",
      "vit.encoder.layer.0.intermediate.dense\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.0.output\n",
      "vit.encoder.layer.0.output.dense\n",
      "vit.encoder.layer.0.output.dropout\n",
      "vit.encoder.layer.0.layernorm_before\n",
      "vit.encoder.layer.0.layernorm_after\n",
      "vit.encoder.layer.1\n",
      "vit.encoder.layer.1.attention\n",
      "vit.encoder.layer.1.attention.attention\n",
      "vit.encoder.layer.1.attention.attention.query\n",
      "vit.encoder.layer.1.attention.attention.key\n",
      "vit.encoder.layer.1.attention.attention.value\n",
      "vit.encoder.layer.1.attention.attention.dropout\n",
      "vit.encoder.layer.1.attention.output\n",
      "vit.encoder.layer.1.attention.output.dense\n",
      "vit.encoder.layer.1.attention.output.dropout\n",
      "vit.encoder.layer.1.intermediate\n",
      "vit.encoder.layer.1.intermediate.dense\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.1.output\n",
      "vit.encoder.layer.1.output.dense\n",
      "vit.encoder.layer.1.output.dropout\n",
      "vit.encoder.layer.1.layernorm_before\n",
      "vit.encoder.layer.1.layernorm_after\n",
      "vit.encoder.layer.2\n",
      "vit.encoder.layer.2.attention\n",
      "vit.encoder.layer.2.attention.attention\n",
      "vit.encoder.layer.2.attention.attention.query\n",
      "vit.encoder.layer.2.attention.attention.key\n",
      "vit.encoder.layer.2.attention.attention.value\n",
      "vit.encoder.layer.2.attention.attention.dropout\n",
      "vit.encoder.layer.2.attention.output\n",
      "vit.encoder.layer.2.attention.output.dense\n",
      "vit.encoder.layer.2.attention.output.dropout\n",
      "vit.encoder.layer.2.intermediate\n",
      "vit.encoder.layer.2.intermediate.dense\n",
      "vit.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.2.output\n",
      "vit.encoder.layer.2.output.dense\n",
      "vit.encoder.layer.2.output.dropout\n",
      "vit.encoder.layer.2.layernorm_before\n",
      "vit.encoder.layer.2.layernorm_after\n",
      "vit.encoder.layer.3\n",
      "vit.encoder.layer.3.attention\n",
      "vit.encoder.layer.3.attention.attention\n",
      "vit.encoder.layer.3.attention.attention.query\n",
      "vit.encoder.layer.3.attention.attention.key\n",
      "vit.encoder.layer.3.attention.attention.value\n",
      "vit.encoder.layer.3.attention.attention.dropout\n",
      "vit.encoder.layer.3.attention.output\n",
      "vit.encoder.layer.3.attention.output.dense\n",
      "vit.encoder.layer.3.attention.output.dropout\n",
      "vit.encoder.layer.3.intermediate\n",
      "vit.encoder.layer.3.intermediate.dense\n",
      "vit.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.3.output\n",
      "vit.encoder.layer.3.output.dense\n",
      "vit.encoder.layer.3.output.dropout\n",
      "vit.encoder.layer.3.layernorm_before\n",
      "vit.encoder.layer.3.layernorm_after\n",
      "vit.encoder.layer.4\n",
      "vit.encoder.layer.4.attention\n",
      "vit.encoder.layer.4.attention.attention\n",
      "vit.encoder.layer.4.attention.attention.query\n",
      "vit.encoder.layer.4.attention.attention.key\n",
      "vit.encoder.layer.4.attention.attention.value\n",
      "vit.encoder.layer.4.attention.attention.dropout\n",
      "vit.encoder.layer.4.attention.output\n",
      "vit.encoder.layer.4.attention.output.dense\n",
      "vit.encoder.layer.4.attention.output.dropout\n",
      "vit.encoder.layer.4.intermediate\n",
      "vit.encoder.layer.4.intermediate.dense\n",
      "vit.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.4.output\n",
      "vit.encoder.layer.4.output.dense\n",
      "vit.encoder.layer.4.output.dropout\n",
      "vit.encoder.layer.4.layernorm_before\n",
      "vit.encoder.layer.4.layernorm_after\n",
      "vit.encoder.layer.5\n",
      "vit.encoder.layer.5.attention\n",
      "vit.encoder.layer.5.attention.attention\n",
      "vit.encoder.layer.5.attention.attention.query\n",
      "vit.encoder.layer.5.attention.attention.key\n",
      "vit.encoder.layer.5.attention.attention.value\n",
      "vit.encoder.layer.5.attention.attention.dropout\n",
      "vit.encoder.layer.5.attention.output\n",
      "vit.encoder.layer.5.attention.output.dense\n",
      "vit.encoder.layer.5.attention.output.dropout\n",
      "vit.encoder.layer.5.intermediate\n",
      "vit.encoder.layer.5.intermediate.dense\n",
      "vit.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.5.output\n",
      "vit.encoder.layer.5.output.dense\n",
      "vit.encoder.layer.5.output.dropout\n",
      "vit.encoder.layer.5.layernorm_before\n",
      "vit.encoder.layer.5.layernorm_after\n",
      "vit.encoder.layer.6\n",
      "vit.encoder.layer.6.attention\n",
      "vit.encoder.layer.6.attention.attention\n",
      "vit.encoder.layer.6.attention.attention.query\n",
      "vit.encoder.layer.6.attention.attention.key\n",
      "vit.encoder.layer.6.attention.attention.value\n",
      "vit.encoder.layer.6.attention.attention.dropout\n",
      "vit.encoder.layer.6.attention.output\n",
      "vit.encoder.layer.6.attention.output.dense\n",
      "vit.encoder.layer.6.attention.output.dropout\n",
      "vit.encoder.layer.6.intermediate\n",
      "vit.encoder.layer.6.intermediate.dense\n",
      "vit.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.6.output\n",
      "vit.encoder.layer.6.output.dense\n",
      "vit.encoder.layer.6.output.dropout\n",
      "vit.encoder.layer.6.layernorm_before\n",
      "vit.encoder.layer.6.layernorm_after\n",
      "vit.encoder.layer.7\n",
      "vit.encoder.layer.7.attention\n",
      "vit.encoder.layer.7.attention.attention\n",
      "vit.encoder.layer.7.attention.attention.query\n",
      "vit.encoder.layer.7.attention.attention.key\n",
      "vit.encoder.layer.7.attention.attention.value\n",
      "vit.encoder.layer.7.attention.attention.dropout\n",
      "vit.encoder.layer.7.attention.output\n",
      "vit.encoder.layer.7.attention.output.dense\n",
      "vit.encoder.layer.7.attention.output.dropout\n",
      "vit.encoder.layer.7.intermediate\n",
      "vit.encoder.layer.7.intermediate.dense\n",
      "vit.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.7.output\n",
      "vit.encoder.layer.7.output.dense\n",
      "vit.encoder.layer.7.output.dropout\n",
      "vit.encoder.layer.7.layernorm_before\n",
      "vit.encoder.layer.7.layernorm_after\n",
      "vit.encoder.layer.8\n",
      "vit.encoder.layer.8.attention\n",
      "vit.encoder.layer.8.attention.attention\n",
      "vit.encoder.layer.8.attention.attention.query\n",
      "vit.encoder.layer.8.attention.attention.key\n",
      "vit.encoder.layer.8.attention.attention.value\n",
      "vit.encoder.layer.8.attention.attention.dropout\n",
      "vit.encoder.layer.8.attention.output\n",
      "vit.encoder.layer.8.attention.output.dense\n",
      "vit.encoder.layer.8.attention.output.dropout\n",
      "vit.encoder.layer.8.intermediate\n",
      "vit.encoder.layer.8.intermediate.dense\n",
      "vit.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.8.output\n",
      "vit.encoder.layer.8.output.dense\n",
      "vit.encoder.layer.8.output.dropout\n",
      "vit.encoder.layer.8.layernorm_before\n",
      "vit.encoder.layer.8.layernorm_after\n",
      "vit.encoder.layer.9\n",
      "vit.encoder.layer.9.attention\n",
      "vit.encoder.layer.9.attention.attention\n",
      "vit.encoder.layer.9.attention.attention.query\n",
      "vit.encoder.layer.9.attention.attention.key\n",
      "vit.encoder.layer.9.attention.attention.value\n",
      "vit.encoder.layer.9.attention.attention.dropout\n",
      "vit.encoder.layer.9.attention.output\n",
      "vit.encoder.layer.9.attention.output.dense\n",
      "vit.encoder.layer.9.attention.output.dropout\n",
      "vit.encoder.layer.9.intermediate\n",
      "vit.encoder.layer.9.intermediate.dense\n",
      "vit.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.9.output\n",
      "vit.encoder.layer.9.output.dense\n",
      "vit.encoder.layer.9.output.dropout\n",
      "vit.encoder.layer.9.layernorm_before\n",
      "vit.encoder.layer.9.layernorm_after\n",
      "vit.encoder.layer.10\n",
      "vit.encoder.layer.10.attention\n",
      "vit.encoder.layer.10.attention.attention\n",
      "vit.encoder.layer.10.attention.attention.query\n",
      "vit.encoder.layer.10.attention.attention.key\n",
      "vit.encoder.layer.10.attention.attention.value\n",
      "vit.encoder.layer.10.attention.attention.dropout\n",
      "vit.encoder.layer.10.attention.output\n",
      "vit.encoder.layer.10.attention.output.dense\n",
      "vit.encoder.layer.10.attention.output.dropout\n",
      "vit.encoder.layer.10.intermediate\n",
      "vit.encoder.layer.10.intermediate.dense\n",
      "vit.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.10.output\n",
      "vit.encoder.layer.10.output.dense\n",
      "vit.encoder.layer.10.output.dropout\n",
      "vit.encoder.layer.10.layernorm_before\n",
      "vit.encoder.layer.10.layernorm_after\n",
      "vit.encoder.layer.11\n",
      "vit.encoder.layer.11.attention\n",
      "vit.encoder.layer.11.attention.attention\n",
      "vit.encoder.layer.11.attention.attention.query\n",
      "vit.encoder.layer.11.attention.attention.key\n",
      "vit.encoder.layer.11.attention.attention.value\n",
      "vit.encoder.layer.11.attention.attention.dropout\n",
      "vit.encoder.layer.11.attention.output\n",
      "vit.encoder.layer.11.attention.output.dense\n",
      "vit.encoder.layer.11.attention.output.dropout\n",
      "vit.encoder.layer.11.intermediate\n",
      "vit.encoder.layer.11.intermediate.dense\n",
      "vit.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.11.output\n",
      "vit.encoder.layer.11.output.dense\n",
      "vit.encoder.layer.11.output.dropout\n",
      "vit.encoder.layer.11.layernorm_before\n",
      "vit.encoder.layer.11.layernorm_after\n",
      "vit.layernorm\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0844da53",
   "metadata": {},
   "source": [
    "# Grad-CAM for ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96b4f2",
   "metadata": {},
   "source": [
    "This notebook uses adjusted code from: https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef13fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568a3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = \"../dataset/test/Healthy/0c1667a2-61d7-4dee-b4d9-0d141a1ceb20___Mt.N.V_HL 9127_new30degFlipLR.JPG\"\n",
    "augmented_path = \"../augmented-dataset/test/Black Rot/0aff8add-93ad-4099-97ae-23515744e620___FAM_B.Rot 0748_flipLR_aug2.png\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dirs = [\"../final-models/hp-vit-base\", \"../final-models/aug-hp-vit-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe78fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(x, height=14, width=14):\n",
    "    if isinstance(x, tuple):\n",
    "        x = x[0]\n",
    "    x = x[:, 1:, :].reshape(x.size(0), height, width, x.size(2))\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        num_labels=4, \n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load('../final-models/aug-hp-vit-base/model_lr0.0001_bs16.pth', map_location=device))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_img_display shape: (224, 224, 3) range: 0.0 1.0\n",
      "grayscale_cam shape: (224, 224) range: 0.0 0.9999999\n"
     ]
    }
   ],
   "source": [
    "target_layer = model.vit.encoder.layer[9].output.dense\n",
    "\n",
    "class ViTWrapper(torch.nn.Module):\n",
    "    def __init__(self, vit_model):\n",
    "        super(ViTWrapper, self).__init__()\n",
    "        self.vit_model = vit_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit_model(x).logits\n",
    "\n",
    "wrapped_model = ViTWrapper(model)\n",
    "\n",
    "cam = GradCAM(model=wrapped_model, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "\n",
    "rgb_img = cv2.imread(augmented_path)[:, :, ::-1]\n",
    "rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "rgb_img_display = np.float32(rgb_img) / 255.0\n",
    "input_tensor = preprocess_image(rgb_img_display.copy() , mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5]).to(device)\n",
    "\n",
    "targets = None\n",
    "\n",
    "cam.batch_size = 16\n",
    "\n",
    "# grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth=True)[0]\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "\n",
    "\n",
    "cam_image = show_cam_on_image(rgb_img_display, grayscale_cam, use_rgb=False)\n",
    "cv2.imwrite(f'grad_cam.jpg', cam_image)\n",
    "\n",
    "print(\"rgb_img_display shape:\", rgb_img_display.shape, \"range:\", rgb_img_display.min(), rgb_img_display.max())\n",
    "print(\"grayscale_cam shape:\", grayscale_cam.shape, \"range:\", grayscale_cam.min(), grayscale_cam.max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

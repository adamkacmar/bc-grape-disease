{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0844da53",
   "metadata": {},
   "source": [
    "# Grad-CAM for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef13fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568a3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = \"../dataset/test/Healthy/0c1667a2-61d7-4dee-b4d9-0d141a1ceb20___Mt.N.V_HL 9127_new30degFlipLR.JPG\"\n",
    "augmented_path = \"../augmented-dataset/test/Black Rot/0aff8add-93ad-4099-97ae-23515744e620___FAM_B.Rot 0748_flipLR_aug2.png\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dirs = [\"../final-models/hp-vit-base\", \"../final-models/aug-hp-vit-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fe78fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5336\\463729187.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../final-models/aug-hp-vit-base/model_lr0.0001_bs16.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x ViTLayer(\n",
      "      (attention): ViTSdpaAttention(\n",
      "        (attention): ViTSdpaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): ViTSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): ViTIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): ViTOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def reshape_transform(x, height=14, width=14):\n",
    "    if isinstance(x, tuple):\n",
    "        x = x[0]\n",
    "    x = x[:, 1:, :].reshape(x.size(0), height, width, x.size(2))\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python vit_gradcam.py --image-path <path_to_image>\n",
    "    Example usage of using cam-methods on a VIT network.\n",
    "\n",
    "    \"\"\"\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        num_labels=4, \n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the fine-tuned weights from your checkpoint\n",
    "    model.load_state_dict(torch.load('../final-models/aug-hp-vit-base/model_lr0.0001_bs16.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    print(model.vit.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b6a7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_img_display shape: (224, 224, 3) range: 0.0 1.0\n",
      "grayscale_cam shape: (224, 224) range: 0.0 0.9999999\n"
     ]
    }
   ],
   "source": [
    "target_layer = model.vit.encoder.layer[9].output.dense\n",
    "\n",
    "class ViTWrapper(torch.nn.Module):\n",
    "    def __init__(self, vit_model):\n",
    "        super(ViTWrapper, self).__init__()\n",
    "        self.vit_model = vit_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit_model(x).logits  # returns just the raw logits\n",
    "\n",
    "wrapped_model = ViTWrapper(model)\n",
    "\n",
    "cam = GradCAM(model=wrapped_model, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "\n",
    "rgb_img = cv2.imread(augmented_path)[:, :, ::-1]\n",
    "rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "rgb_img_display = np.float32(rgb_img) / 255.0\n",
    "input_tensor = preprocess_image(rgb_img_display.copy() , mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5]).to(device)\n",
    "\n",
    "# If None, returns the map for the highest scoring category.\n",
    "# Otherwise, targets the requested category.\n",
    "targets = None\n",
    "\n",
    "\n",
    "# AblationCAM and ScoreCAM have batched implementations.\n",
    "# You can override the internal batch size for faster computation.\n",
    "cam.batch_size = 16\n",
    "\n",
    "# grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth=True)[0]\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "\n",
    "\n",
    "cam_image = show_cam_on_image(rgb_img_display, grayscale_cam, use_rgb=False)\n",
    "cv2.imwrite(f'grad_cam.jpg', cam_image)\n",
    "\n",
    "print(\"rgb_img_display shape:\", rgb_img_display.shape, \"range:\", rgb_img_display.min(), rgb_img_display.max())\n",
    "print(\"grayscale_cam shape:\", grayscale_cam.shape, \"range:\", grayscale_cam.min(), grayscale_cam.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b90755da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vit\n",
      "vit.embeddings\n",
      "vit.embeddings.patch_embeddings\n",
      "vit.embeddings.patch_embeddings.projection\n",
      "vit.embeddings.dropout\n",
      "vit.encoder\n",
      "vit.encoder.layer\n",
      "vit.encoder.layer.0\n",
      "vit.encoder.layer.0.attention\n",
      "vit.encoder.layer.0.attention.attention\n",
      "vit.encoder.layer.0.attention.attention.query\n",
      "vit.encoder.layer.0.attention.attention.key\n",
      "vit.encoder.layer.0.attention.attention.value\n",
      "vit.encoder.layer.0.attention.attention.dropout\n",
      "vit.encoder.layer.0.attention.output\n",
      "vit.encoder.layer.0.attention.output.dense\n",
      "vit.encoder.layer.0.attention.output.dropout\n",
      "vit.encoder.layer.0.intermediate\n",
      "vit.encoder.layer.0.intermediate.dense\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.0.output\n",
      "vit.encoder.layer.0.output.dense\n",
      "vit.encoder.layer.0.output.dropout\n",
      "vit.encoder.layer.0.layernorm_before\n",
      "vit.encoder.layer.0.layernorm_after\n",
      "vit.encoder.layer.1\n",
      "vit.encoder.layer.1.attention\n",
      "vit.encoder.layer.1.attention.attention\n",
      "vit.encoder.layer.1.attention.attention.query\n",
      "vit.encoder.layer.1.attention.attention.key\n",
      "vit.encoder.layer.1.attention.attention.value\n",
      "vit.encoder.layer.1.attention.attention.dropout\n",
      "vit.encoder.layer.1.attention.output\n",
      "vit.encoder.layer.1.attention.output.dense\n",
      "vit.encoder.layer.1.attention.output.dropout\n",
      "vit.encoder.layer.1.intermediate\n",
      "vit.encoder.layer.1.intermediate.dense\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.1.output\n",
      "vit.encoder.layer.1.output.dense\n",
      "vit.encoder.layer.1.output.dropout\n",
      "vit.encoder.layer.1.layernorm_before\n",
      "vit.encoder.layer.1.layernorm_after\n",
      "vit.encoder.layer.2\n",
      "vit.encoder.layer.2.attention\n",
      "vit.encoder.layer.2.attention.attention\n",
      "vit.encoder.layer.2.attention.attention.query\n",
      "vit.encoder.layer.2.attention.attention.key\n",
      "vit.encoder.layer.2.attention.attention.value\n",
      "vit.encoder.layer.2.attention.attention.dropout\n",
      "vit.encoder.layer.2.attention.output\n",
      "vit.encoder.layer.2.attention.output.dense\n",
      "vit.encoder.layer.2.attention.output.dropout\n",
      "vit.encoder.layer.2.intermediate\n",
      "vit.encoder.layer.2.intermediate.dense\n",
      "vit.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.2.output\n",
      "vit.encoder.layer.2.output.dense\n",
      "vit.encoder.layer.2.output.dropout\n",
      "vit.encoder.layer.2.layernorm_before\n",
      "vit.encoder.layer.2.layernorm_after\n",
      "vit.encoder.layer.3\n",
      "vit.encoder.layer.3.attention\n",
      "vit.encoder.layer.3.attention.attention\n",
      "vit.encoder.layer.3.attention.attention.query\n",
      "vit.encoder.layer.3.attention.attention.key\n",
      "vit.encoder.layer.3.attention.attention.value\n",
      "vit.encoder.layer.3.attention.attention.dropout\n",
      "vit.encoder.layer.3.attention.output\n",
      "vit.encoder.layer.3.attention.output.dense\n",
      "vit.encoder.layer.3.attention.output.dropout\n",
      "vit.encoder.layer.3.intermediate\n",
      "vit.encoder.layer.3.intermediate.dense\n",
      "vit.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.3.output\n",
      "vit.encoder.layer.3.output.dense\n",
      "vit.encoder.layer.3.output.dropout\n",
      "vit.encoder.layer.3.layernorm_before\n",
      "vit.encoder.layer.3.layernorm_after\n",
      "vit.encoder.layer.4\n",
      "vit.encoder.layer.4.attention\n",
      "vit.encoder.layer.4.attention.attention\n",
      "vit.encoder.layer.4.attention.attention.query\n",
      "vit.encoder.layer.4.attention.attention.key\n",
      "vit.encoder.layer.4.attention.attention.value\n",
      "vit.encoder.layer.4.attention.attention.dropout\n",
      "vit.encoder.layer.4.attention.output\n",
      "vit.encoder.layer.4.attention.output.dense\n",
      "vit.encoder.layer.4.attention.output.dropout\n",
      "vit.encoder.layer.4.intermediate\n",
      "vit.encoder.layer.4.intermediate.dense\n",
      "vit.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.4.output\n",
      "vit.encoder.layer.4.output.dense\n",
      "vit.encoder.layer.4.output.dropout\n",
      "vit.encoder.layer.4.layernorm_before\n",
      "vit.encoder.layer.4.layernorm_after\n",
      "vit.encoder.layer.5\n",
      "vit.encoder.layer.5.attention\n",
      "vit.encoder.layer.5.attention.attention\n",
      "vit.encoder.layer.5.attention.attention.query\n",
      "vit.encoder.layer.5.attention.attention.key\n",
      "vit.encoder.layer.5.attention.attention.value\n",
      "vit.encoder.layer.5.attention.attention.dropout\n",
      "vit.encoder.layer.5.attention.output\n",
      "vit.encoder.layer.5.attention.output.dense\n",
      "vit.encoder.layer.5.attention.output.dropout\n",
      "vit.encoder.layer.5.intermediate\n",
      "vit.encoder.layer.5.intermediate.dense\n",
      "vit.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.5.output\n",
      "vit.encoder.layer.5.output.dense\n",
      "vit.encoder.layer.5.output.dropout\n",
      "vit.encoder.layer.5.layernorm_before\n",
      "vit.encoder.layer.5.layernorm_after\n",
      "vit.encoder.layer.6\n",
      "vit.encoder.layer.6.attention\n",
      "vit.encoder.layer.6.attention.attention\n",
      "vit.encoder.layer.6.attention.attention.query\n",
      "vit.encoder.layer.6.attention.attention.key\n",
      "vit.encoder.layer.6.attention.attention.value\n",
      "vit.encoder.layer.6.attention.attention.dropout\n",
      "vit.encoder.layer.6.attention.output\n",
      "vit.encoder.layer.6.attention.output.dense\n",
      "vit.encoder.layer.6.attention.output.dropout\n",
      "vit.encoder.layer.6.intermediate\n",
      "vit.encoder.layer.6.intermediate.dense\n",
      "vit.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.6.output\n",
      "vit.encoder.layer.6.output.dense\n",
      "vit.encoder.layer.6.output.dropout\n",
      "vit.encoder.layer.6.layernorm_before\n",
      "vit.encoder.layer.6.layernorm_after\n",
      "vit.encoder.layer.7\n",
      "vit.encoder.layer.7.attention\n",
      "vit.encoder.layer.7.attention.attention\n",
      "vit.encoder.layer.7.attention.attention.query\n",
      "vit.encoder.layer.7.attention.attention.key\n",
      "vit.encoder.layer.7.attention.attention.value\n",
      "vit.encoder.layer.7.attention.attention.dropout\n",
      "vit.encoder.layer.7.attention.output\n",
      "vit.encoder.layer.7.attention.output.dense\n",
      "vit.encoder.layer.7.attention.output.dropout\n",
      "vit.encoder.layer.7.intermediate\n",
      "vit.encoder.layer.7.intermediate.dense\n",
      "vit.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.7.output\n",
      "vit.encoder.layer.7.output.dense\n",
      "vit.encoder.layer.7.output.dropout\n",
      "vit.encoder.layer.7.layernorm_before\n",
      "vit.encoder.layer.7.layernorm_after\n",
      "vit.encoder.layer.8\n",
      "vit.encoder.layer.8.attention\n",
      "vit.encoder.layer.8.attention.attention\n",
      "vit.encoder.layer.8.attention.attention.query\n",
      "vit.encoder.layer.8.attention.attention.key\n",
      "vit.encoder.layer.8.attention.attention.value\n",
      "vit.encoder.layer.8.attention.attention.dropout\n",
      "vit.encoder.layer.8.attention.output\n",
      "vit.encoder.layer.8.attention.output.dense\n",
      "vit.encoder.layer.8.attention.output.dropout\n",
      "vit.encoder.layer.8.intermediate\n",
      "vit.encoder.layer.8.intermediate.dense\n",
      "vit.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.8.output\n",
      "vit.encoder.layer.8.output.dense\n",
      "vit.encoder.layer.8.output.dropout\n",
      "vit.encoder.layer.8.layernorm_before\n",
      "vit.encoder.layer.8.layernorm_after\n",
      "vit.encoder.layer.9\n",
      "vit.encoder.layer.9.attention\n",
      "vit.encoder.layer.9.attention.attention\n",
      "vit.encoder.layer.9.attention.attention.query\n",
      "vit.encoder.layer.9.attention.attention.key\n",
      "vit.encoder.layer.9.attention.attention.value\n",
      "vit.encoder.layer.9.attention.attention.dropout\n",
      "vit.encoder.layer.9.attention.output\n",
      "vit.encoder.layer.9.attention.output.dense\n",
      "vit.encoder.layer.9.attention.output.dropout\n",
      "vit.encoder.layer.9.intermediate\n",
      "vit.encoder.layer.9.intermediate.dense\n",
      "vit.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.9.output\n",
      "vit.encoder.layer.9.output.dense\n",
      "vit.encoder.layer.9.output.dropout\n",
      "vit.encoder.layer.9.layernorm_before\n",
      "vit.encoder.layer.9.layernorm_after\n",
      "vit.encoder.layer.10\n",
      "vit.encoder.layer.10.attention\n",
      "vit.encoder.layer.10.attention.attention\n",
      "vit.encoder.layer.10.attention.attention.query\n",
      "vit.encoder.layer.10.attention.attention.key\n",
      "vit.encoder.layer.10.attention.attention.value\n",
      "vit.encoder.layer.10.attention.attention.dropout\n",
      "vit.encoder.layer.10.attention.output\n",
      "vit.encoder.layer.10.attention.output.dense\n",
      "vit.encoder.layer.10.attention.output.dropout\n",
      "vit.encoder.layer.10.intermediate\n",
      "vit.encoder.layer.10.intermediate.dense\n",
      "vit.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.10.output\n",
      "vit.encoder.layer.10.output.dense\n",
      "vit.encoder.layer.10.output.dropout\n",
      "vit.encoder.layer.10.layernorm_before\n",
      "vit.encoder.layer.10.layernorm_after\n",
      "vit.encoder.layer.11\n",
      "vit.encoder.layer.11.attention\n",
      "vit.encoder.layer.11.attention.attention\n",
      "vit.encoder.layer.11.attention.attention.query\n",
      "vit.encoder.layer.11.attention.attention.key\n",
      "vit.encoder.layer.11.attention.attention.value\n",
      "vit.encoder.layer.11.attention.attention.dropout\n",
      "vit.encoder.layer.11.attention.output\n",
      "vit.encoder.layer.11.attention.output.dense\n",
      "vit.encoder.layer.11.attention.output.dropout\n",
      "vit.encoder.layer.11.intermediate\n",
      "vit.encoder.layer.11.intermediate.dense\n",
      "vit.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.11.output\n",
      "vit.encoder.layer.11.output.dense\n",
      "vit.encoder.layer.11.output.dropout\n",
      "vit.encoder.layer.11.layernorm_before\n",
      "vit.encoder.layer.11.layernorm_after\n",
      "vit.layernorm\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
